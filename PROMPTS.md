# LLM Prompt Template

The project includes a placeholder for an optional Large Language Model (LLM) hook.  By default this hook is **disabled**; the deterministic matching engine returns recommendations without calling any external services.  Should you wish to integrate an LLM in the future, the `backend/app/matching/prompts.py` file defines a Jinja template that can be used to generate a prompt for the model.

## Template Structure

The template is a string with placeholders enclosed in double curly braces (`{{ }}`).  When rendering the template, the backend injects the student profile and the preliminary recommendations generated by the ruleâ€‘based engine.  Here is a simplified version:

```jinja
Student Profile:
  Name: {{ student.name }}
  Year Level: {{ student.yearLevel }}
  Interests: {{ student.interests | join(', ') }}
  Strengths: {{ student.strengths | join(', ') }}
  Academic Performance: {{ student.academicPerformance }}

Preliminary Recommendations:
{% for rec in recommendations %}
  - {{ rec.name }} (confidence: {{ '%.2f' % rec.confidence }})
    Why: {{ rec.why }}
    Suggested Subjects: {{ rec.suggestedSubjects | join(', ') }}
    VET Options: {{ rec.vetOptions | join(', ') }}
{% endfor %}

Task:
Based on the student's profile and the preliminary recommendations, suggest refined career pathways and personalised next steps.  Provide a short paragraph explaining the reasoning for each suggestion and highlight any potential subject areas or extracurricular activities that could enhance the student's preparation.
```

## JSON Schema for the Response

If you choose to call an LLM and have it return structured data, ensure the response matches the following schema so that it can be parsed safely:

```json
{
  "recommendations": [
    {
      "careerId": "string",
      "name": "string",
      "confidence": 0.0,
      "why": "string",
      "suggestedSubjects": ["string", ...],
      "vetOptions": ["string", ...],
      "nextSteps": "string"
    },
    ...
  ]
}
```

The backend currently ignores any LLM response and always returns the deterministic recommendations.  To enable the hook, modify `backend/app/routers/recommend.py` to call your LLM service and merge or replace the recommendations accordingly.